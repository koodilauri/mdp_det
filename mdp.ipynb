{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NORTH = 0\n",
    "SOUTH = 1\n",
    "WEST = 2\n",
    "EAST = 3\n",
    "\n",
    "class Robot:\n",
    "    \"\"\"\n",
    "    The world is a MxN grid and your goal is to reach the terminal state.\n",
    "    \n",
    "    For, example, a 4x4 grid could look like this:\n",
    "    \n",
    "    T . . .\n",
    "    . . . .\n",
    "    . o o .\n",
    "    . x o T\n",
    "    \n",
    "    x is your current state, T is the terminal state and o is an obstacle\n",
    "    \n",
    "    You can take four different actions: moving north, south, west or east.\n",
    "    Actions cannot take you off the grid or move through obstacles.\n",
    "    \n",
    "    When taking an action, you will receive a reward depending which state you arrived \n",
    "    (default reward is 0 for terminal state and -1 for other states)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, world_definition = None):\n",
    "        if world_definition is None:\n",
    "            self.rewards = {\"T\": 0.0, \".\": -1.0}\n",
    "            self.world = np.array (\n",
    "                [[\"T\", \".\", \".\", \".\"],\n",
    "                 [\".\", \".\", \".\", \".\"],\n",
    "                 [\".\", \".\", \".\", \".\"],\n",
    "                 [\".\", \".\", \".\", \"T\"]])\n",
    "        else:\n",
    "            self.world, self.rewards = world_definition\n",
    "        \n",
    "        self.shape = self.world.shape\n",
    "        self.MAX_Y = self.shape[0]\n",
    "        self.MAX_X = self.shape[1]\n",
    "    \n",
    "        # State space\n",
    "        self.nSp = np.prod(self.shape)\n",
    "        self.S = [] # Set of all non-terminal states\n",
    "        self.Sp = [] # Set of all states\n",
    "        self.S_start = []\n",
    "        \n",
    "        # Action set\n",
    "        self.A = [NORTH, SOUTH, WEST, EAST]\n",
    "        self.nA = len(self.A)\n",
    "        \n",
    "        self.T = {} # Transition model: T(.,.): S x A --> S\n",
    "        self.R = {} # Reward function: R(.,.): S x A --> R\n",
    "    \n",
    "        for state in range(self.nSp):\n",
    "            self.T[state] = {a: [] for a in self.A} # T[state][action] = T(s,a)\n",
    "            self.R[state] = {a: [] for a in self.A} # R[state][action] = R(s,a)\n",
    "        \n",
    "            if self.is_terminal(state):\n",
    "                self.Sp.append(state)\n",
    "                \n",
    "                for action in self.A:\n",
    "                    new_state_a = state # If state is terminal, we do not move from current state\n",
    "                \n",
    "                    # Get the reward of the updates state from the world\n",
    "                    x, y = self._state2coord(new_state_a)\n",
    "                    reward = self.rewards[self.world[y, x]] # The world is accessed by (row, column)\n",
    "                \n",
    "                    self.T[state][action] = new_state_a\n",
    "                    self.R[state][action] = reward\n",
    "            \n",
    "            else:\n",
    "                self.S.append(state)\n",
    "                \n",
    "                if self._is_start(state):\n",
    "                    self.S_start.append(state)\n",
    "                \n",
    "                for action in self.A:\n",
    "                    new_state_a = self._update_state(state, action) # state + action: agent moves to a new state\n",
    "                \n",
    "                    # Get the reward of the updates state from the world\n",
    "                    x, y = self._state2coord(new_state_a)\n",
    "                    reward = self.rewards[self.world[y, x]] # The world is accessed by (row, column)\n",
    "                \n",
    "                    self.T[state][action] = new_state_a\n",
    "                    self.R[state][action] = reward\n",
    "                \n",
    "                \n",
    "                \n",
    "        self.Sp = self.Sp + self.S\n",
    "        \n",
    "    def take_action(self, state, action):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        new_state_a = self.T[state][action]\n",
    "        reward = self.R[state][action]\n",
    "        \n",
    "        return new_state_a, reward\n",
    "    \n",
    "    def reset(self, random_initial_state = False):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        if random_initial_state:\n",
    "            self._current_state = np.random.choice(self.S, size=1)[0]\n",
    "        else:\n",
    "            self._current_state = self.S_start[0]\n",
    "            \n",
    "        return self._current_state\n",
    "    \n",
    "    def _update_state(self, state, action):\n",
    "        \"\"\"\n",
    "        The origin of the gridworld is in the south-west corner\n",
    "        parameters: \n",
    "            state: index of state s\n",
    "            action: index of action a\n",
    "        \n",
    "        return:\n",
    "            index of the new state s'\n",
    "        \"\"\"\n",
    "        \n",
    "        x, y = self._state2coord(state)\n",
    "        \n",
    "        if action == NORTH:\n",
    "            new_xy = (x,y-1)\n",
    "        elif action == SOUTH:\n",
    "            new_xy = (x,y+1)\n",
    "        elif action == WEST:\n",
    "            new_xy = (x-1,y)\n",
    "        elif action == EAST:\n",
    "            new_xy = (x+1,y)\n",
    "        else:\n",
    "            raise ValueError (\"Invalid action %d.\" % action)\n",
    "            \n",
    "        if self._is_outside_at(new_xy) or self._is_obstacle_at(new_xy):\n",
    "            # If the updated step would be outside the gridworld or on top of an obstacle, the state remains unchainged\n",
    "            return self._coord2state ((x, y)) \n",
    "        else:\n",
    "            return self._coord2state(new_xy)\n",
    "        \n",
    "    def _is_start(self, state):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        x, y = self._state2coord (state)\n",
    "        return self.world[y, x] == \"S\"\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"\n",
    "        Determine if current state is a terminal state\n",
    "        parameters: \n",
    "            state: index of state s\n",
    "        return:\n",
    "            boolean: true if state is terminal, false otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        x, y = self._state2coord(state)\n",
    "        return self.world[y][x] == \"T\"\n",
    "    \n",
    "    def _is_outside_at(self, xy):\n",
    "        \"\"\"\n",
    "        Determine if current state is a outside gridworld boundaries\n",
    "        parameters: \n",
    "            state: index of state s\n",
    "        return:\n",
    "            boolean: true if state is outside the gridworld, false otherwise\n",
    "        \"\"\"\n",
    "        x = xy[0]\n",
    "        y = xy[1]\n",
    "        \n",
    "        return y < 0 or x < 0 or x >= self.MAX_X or y >= self.MAX_Y\n",
    "    \n",
    "    def _is_obstacle_at(self, xy):\n",
    "        \"\"\"\n",
    "        Determine if there is an obstacle at the current state\n",
    "        parameters: \n",
    "            state: index of state s\n",
    "        return:\n",
    "            boolean: true if current coordinates have an obstacle, false otherwise\n",
    "        \"\"\"\n",
    "        y, x = xy[1], xy[0]\n",
    "        \n",
    "        return self.world[y][x] == \"o\"\n",
    "        \n",
    "    def _state2coord(self, state):\n",
    "        \"\"\"\n",
    "        Converts a flat index (state) into a tuple of coordinate arrays.\n",
    "        parameters: \n",
    "            state: index of state s\n",
    "        return:\n",
    "            tuple: state coordinates (x,y)\n",
    "        \"\"\"\n",
    "        \n",
    "        yx = np.unravel_index(state, self.shape, order=\"C\")\n",
    "        return (yx[1], yx[0])\n",
    "    \n",
    "    def _coord2state(self, xy):\n",
    "        \"\"\"\n",
    "        Converts a tuple of index arrays into an array of flat indices, applying boundary modes to the multi-index.\n",
    "        parameters: \n",
    "            tuple: state coordinates (x,y)\n",
    "        return:\n",
    "            state: index of state s\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.ravel_multi_index((xy[1], xy[0]), self.shape, order=\"C\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value_for_action(env, s, V, discount_factor):\n",
    "    \"\"\"\n",
    "    Function to calculate the value of all actions in a given state\n",
    "    parameters: \n",
    "        env: instance of the Robot class\n",
    "        s: index of current state s\n",
    "        V: vector (shape (nSp,)) with the current value function estimate\n",
    "        discoun_factor: discount factor gamma\n",
    "    return:\n",
    "        vector (shape (nA,)) containing the value for each action\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vector to store all the values\n",
    "    A = np.zeros(env.nA)\n",
    "    \n",
    "    # Calculate the value for each action a in A\n",
    "    for action in range(0, env.nA):\n",
    "        # Get the reward from the reward function R(s,a) of the environment\n",
    "        reward = env.R[s][action]\n",
    "        # Get the new state from the transition model T(s,a)\n",
    "        new_state_a = env.T[s][action]\n",
    "        # Calculate R(s,a) + \\gamma * v(T(s,a))\n",
    "        A[action] = reward + discount_factor * V[new_state_a]\n",
    "    \n",
    "    return A\n",
    "\n",
    "def value_iteration(env, theta=0.0001, discount_factor = 0.9):\n",
    "    \"\"\"\n",
    "    Value-iteration algorithm\n",
    "    parameters: \n",
    "        env: instance of the Robot class\n",
    "        theta: convergence threshold\n",
    "        discoun_factor: discount factor gamma\n",
    "    return:\n",
    "        vector (shape (nSp,)) containing the value-function estimate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize value-function V(s) for each state to be zero\n",
    "    V = np.zeros(env.nSp)\n",
    "    k = 0\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        \n",
    "        for s in env.S:\n",
    "            v = V[s]\n",
    "            \n",
    "            # Calculate R(s,a) + \\gamma * v(T(s,a)) for each action a in A\n",
    "            A = calculate_value_for_action(env, s, V, discount_factor)\n",
    "            \n",
    "            # Find the best action from A and assign its value to V(s)\n",
    "            V[s] = np.amax(A)\n",
    "            \n",
    "            # Update the delta\n",
    "            delta = max(delta, abs(V[s] - v))\n",
    "        \n",
    "        k = k + 1\n",
    "        \n",
    "        # Check if we should stop\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    print (\"Iterations (value iteration): %d\" % k)\n",
    "    \n",
    "    return V\n",
    "\n",
    "def extract_policy(env, V, discount_factor = 0.9):\n",
    "    \"\"\"\n",
    "    Find the optimal policy using a greedy strategy.\n",
    "\n",
    "    If two actions have the same value for a certain state s,\n",
    "    the action is chosen in the order of their indices.\n",
    "\n",
    "    parameters: \n",
    "        env: instance of the Robot class\n",
    "        V: vector (shape (nSp,)) with the current value function estimate\n",
    "        discoun_factor: discount factor gamma\n",
    "    return:\n",
    "        policy: (shape (nSp,)) has the optimal action a in state s\n",
    "        Q: shape (nSp, nA), action-value-function containing the value of action a in state s\n",
    "    \"\"\"\n",
    "    # pi(.): S --> A\n",
    "    policy = np.zeros(env.nSp)\n",
    "    \n",
    "    # Q is the action-value-function\n",
    "    Q = np.zeros([env.nSp, env.nA])\n",
    "    \n",
    "    for s in env.S:\n",
    "        # Calculate R(s,a) + \\gamma * v(T(s,a)) for each action a in A\n",
    "        A = calculate_value_for_action(env, s, V, discount_factor)\n",
    "        # Get the index of the best action\n",
    "        best_action = np.argmax(A)\n",
    "        # The best action is always chosen for each state s\n",
    "        policy[s] = best_action\n",
    "        \n",
    "        Q[s,:] = A\n",
    "    \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations (value iteration): 89\n",
      "World: \n",
      "[['T' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' '.' 'S']\n",
      " ['.' '.' 'o' 'o' 'o' 'o']\n",
      " ['.' '.' 'o' '.' '.' '.']\n",
      " ['.' '.' 'o' '.' '.' '.']\n",
      " ['.' '.' 'o' '.' '.' '.']]\n",
      "Optimal policy: \n",
      "[[0. 2. 2. 2. 2. 2.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 0. 0.]]\n",
      "Actions: NORTH=0, SOUTH=1, WEST=2, EAST=3\n"
     ]
    }
   ],
   "source": [
    "def create_example_world(type = \"4x4_world\"):\n",
    "    if type == \"4x4_world\":\n",
    "        rewards = {\"T\": 0.0, \".\": -1.0, \"o\": -1.0, \"S\": -1.0}\n",
    "        world = np.array (\n",
    "            [[\"T\", \".\", \".\", \".\"],\n",
    "             [\"o\", \".\", \".\", \"o\"],\n",
    "             [\"S\", \".\", \".\", \".\"],\n",
    "             [\".\", \".\", \".\", \"T\"]])\n",
    "\n",
    "    elif type == \"obstacles\":\n",
    "        rewards = {\"T\": 0.0, \".\": -1.0, \"o\": -1.0, \"S\": -1.0}\n",
    "        world = np.array (\n",
    "            [[\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "             [\".\", \"o\", \".\", \".\", \"S\", \".\"],\n",
    "             [\".\", \"o\", \"o\", \".\", \".\", \".\"],\n",
    "             [\".\", \".\", \"o\", \".\", \".\", \".\"],\n",
    "             [\".\", \".\", \"o\", \"o\", \"o\", \"o\"],\n",
    "             [\".\", \".\", \".\", \".\", \".\", \"T\"]])\n",
    "\n",
    "    elif type == \"6x6_world_blocked\":\n",
    "        rewards = {\"T\": 0.0, \".\": -1.0, \"o\": -1.0, \"S\": -1.0}\n",
    "        world = np.array (\n",
    "            [[\"T\", \".\", \".\", \".\", \".\", \".\"],\n",
    "             [\".\", \".\", \".\", \".\", \".\", \"S\"],\n",
    "             [\".\", \".\", \"o\", \"o\", \"o\", \"o\"],\n",
    "             [\".\", \".\", \"o\", \".\", \".\", \".\"],\n",
    "             [\".\", \".\", \"o\", \".\", \".\", \".\"],\n",
    "             [\".\", \".\", \"o\", \".\", \".\", \".\"]])\n",
    "\n",
    "    elif type == \"6x4_world_2\":\n",
    "        rewards = {\"T\": 0.0, \".\": -1.0, \"o\": -1.0, \"x\": -5.0, \"S\": -1.0}\n",
    "        world = np.array (\n",
    "            [[\"T\", \".\", \".\", \".\"],\n",
    "             [\"x\", \"x\", \"x\", \"x\"],\n",
    "             [\"x\", \"x\", \".\", \"x\"],\n",
    "             [\"x\", \"x\", \".\", \"x\"],\n",
    "             [\"x\", \"x\", \".\", \"x\"],\n",
    "             [\"S\", \".\", \".\", \".\"]])\n",
    "    else:\n",
    "        raise ValueError (\"Invalid example world type: %s\" % type)\n",
    "\n",
    "    return world, rewards\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "for type in [\"6x6_world_blocked\"]:\n",
    "    gworld = create_example_world(type)\n",
    "    env = Robot(gworld)\n",
    "\n",
    "    V = value_iteration(env, discount_factor = discount_factor)\n",
    "    policy, Q = extract_policy(env, V, discount_factor = discount_factor)\n",
    "\n",
    "    print(\"World: \")\n",
    "    print(np.array(env.world).reshape(env.shape))\n",
    "    print(\"Optimal policy: \")\n",
    "    print(policy.reshape(env.shape))\n",
    "    print(\"Actions: NORTH=0, SOUTH=1, WEST=2, EAST=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(robot, init_state, discount_factor=1.0, num_iterations=100000):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Q = np.zeros((robot.nSp, robot.nA))\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    cur_state = init_state\n",
    "    while i <= num_iterations:\n",
    "        i += 1\n",
    "        \n",
    "        # Choose the next action according to the policy (random policy)\n",
    "        action = np.random.choice(robot.A, 1)[0]\n",
    "        \n",
    "        # Get the state and reward by taking action from the current state\n",
    "        new_state, reward = robot.take_action(cur_state, action)\n",
    "        \n",
    "        # Update current Q value for current state and action\n",
    "        Q[cur_state][action] = reward + discount_factor * np.max(Q[new_state])\n",
    "        \n",
    "        \n",
    "        \n",
    "        if robot.is_terminal(new_state):\n",
    "            cur_state = robot.reset()\n",
    "        else:\n",
    "            cur_state = new_state\n",
    "        \n",
    "    return Q\n",
    "\n",
    "def get_optimal_path(robot, Q):\n",
    "    \n",
    "    path = []\n",
    "    init_state = robot.S_start[0]\n",
    "    \n",
    "    cur_state = init_state\n",
    "    path.append(cur_state)\n",
    "    \n",
    "    while not robot.is_terminal(cur_state):\n",
    "        action = np.argmax(Q[cur_state]) # get best action\n",
    "        cur_state, r = robot.take_action(cur_state, action)\n",
    "        path.append(cur_state)\n",
    "        \n",
    "    return path\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import svgwrite\n",
    "\n",
    "def best_directions(x,y, Q, robot):\n",
    "    s = robot._coord2state((x,y))\n",
    "    actions = np.argwhere(Q[s] == np.amax(Q[s]))\n",
    "    return actions.flatten().tolist()\n",
    "\n",
    "\n",
    "def draw_policy(robot, Q):\n",
    "    square_size = 90\n",
    "    width = 3\n",
    "    half_size = np.ceil(square_size/2)\n",
    "    y,x = robot.shape\n",
    "    cellcolor = \"white\"\n",
    "    white = \"white\"\n",
    "    T_color = \"grey\"\n",
    "    o_color = \"black\"\n",
    "    S_color = \"green\"\n",
    "    x_color = \"red\"\n",
    "\n",
    "    dwg = svgwrite.Drawing('policy.svg', profile='full')\n",
    "    for i in range(0,x):\n",
    "        for w in range(0,y):\n",
    "            if robot.world[w,i] == \"S\":\n",
    "                cellcolor = S_color\n",
    "            elif robot.world[w,i] == \"T\":\n",
    "                cellcolor = T_color\n",
    "            elif robot.world[w,i] == \"o\":\n",
    "                cellcolor = o_color\n",
    "            elif robot.world[w,i] == \"x\":\n",
    "                cellcolor = x_color\n",
    "            else:\n",
    "                cellcolor = white\n",
    "            dwg.add(dwg.rect((i*square_size, w*square_size), (square_size, square_size), fill=cellcolor, stroke='black'))\n",
    "\n",
    "            if robot.world[w, i] in [\"T\", \"o\"]:\n",
    "                    continue\n",
    "\n",
    "            center_x = i * square_size + half_size\n",
    "            center_y = w * square_size + half_size\n",
    "\n",
    "            a = best_directions(i, w, Q, robot)\n",
    "            for q in range(0, robot.nA):\n",
    "                if NORTH in a:\n",
    "                    dwg.add(dwg.polyline([(center_x, center_y), (center_x, center_y - half_size)], stroke = \"black\", stroke_width = width, fill = \"none\"))\n",
    "                if SOUTH in a:\n",
    "                    dwg.add(dwg.polyline([(center_x, center_y), (center_x, center_y + half_size)], stroke = \"black\", stroke_width = width, fill = \"none\"))\n",
    "                if WEST in a:\n",
    "                    dwg.add(dwg.polyline([(center_x, center_y), (center_x - half_size, center_y )], stroke = \"black\", stroke_width = width, fill = \"none\"))\n",
    "                if EAST in a:\n",
    "                    dwg.add(dwg.polyline([(center_x, center_y), (center_x + half_size, center_y)], stroke = \"black\", stroke_width = width, fill = \"none\"))\n",
    "\n",
    "    dwg.add(dwg.text('Test', insert=(0, 0.2), fill='red'))\n",
    "    dwg.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-value function: \n",
      "[[ 0.       0.       0.       0.     ]\n",
      " [-1.      -1.9      0.      -1.9    ]\n",
      " [-1.9     -2.71    -1.      -2.71   ]\n",
      " [-2.71    -3.439   -1.9     -3.439  ]\n",
      " [-3.439   -4.0951  -2.71    -4.0951 ]\n",
      " [-4.0951  -4.68559 -3.439   -4.0951 ]\n",
      " [ 0.      -1.9     -1.      -1.9    ]\n",
      " [-1.      -2.71    -1.      -2.71   ]\n",
      " [-1.9     -2.71    -1.9     -3.439  ]\n",
      " [-2.71    -3.439   -2.71    -4.0951 ]\n",
      " [-3.439   -4.0951  -3.439   -4.68559]\n",
      " [-4.0951  -4.68559 -4.0951  -4.68559]\n",
      " [-1.      -2.71    -1.9     -2.71   ]\n",
      " [-1.9     -3.439   -1.9     -2.71   ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [-1.9     -3.439   -2.71    -3.439  ]\n",
      " [-2.71    -4.0951  -2.71    -3.439  ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [-2.71    -4.0951  -3.439   -4.0951 ]\n",
      " [-3.439   -4.68559 -3.439   -4.0951 ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [-3.439   -4.0951  -4.0951  -4.68559]\n",
      " [-4.0951  -4.68559 -4.0951  -4.68559]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.     ]]\n",
      "Optimal path: \n",
      "[(5, 1), (5, 0), (4, 0), (3, 0), (2, 0), (1, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "discount_factor = 0.9\n",
    "robot = Robot(create_example_world(\"6x6_world_blocked\")) # 4x4_world, 6x4_world_2 6x6_world_blocked\n",
    "Q = q_learning(robot, robot.S_start[0], discount_factor)\n",
    "print (\"Action-value function: \")\n",
    "print(Q)\n",
    "path = get_optimal_path(robot, Q)\n",
    "print (\"Optimal path: \")\n",
    "print(list(map(robot._state2coord, path)))\n",
    "draw_policy(robot,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
